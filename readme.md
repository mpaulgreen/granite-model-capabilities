```
ollama serve & # When using granite3.3:8b
```

# Examples here demontsrates the above model served locally on a M3 mac can perform
- Text Classification
- Text Translation
- Text Extraction
- Text Summarization
- Control response length
- Advanced Use Case
    - Thinking capabilities
    - Function calling
    - RAG use case using a local embedding model
- [Agentic app using CrewAI and ollama/llama3.1](https://github.com/mpaulgreen/agent_crewai-ollama/blob/main/output.md)